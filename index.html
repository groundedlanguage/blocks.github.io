
<html>
<head>
<title>ISI Language Grounding Data Set</title>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-8024377-7', 'auto');
  ga('send', 'pageview');
</script>
</head>
<body bgcolor=808080>
<center>
<br><br>
<table width=815 bgcolor=FFFFFF>
<tr><td colspan=3><center><h1>ISI Language Grounding Data Set</h1></center></td></tr>
<tr><td colspan=3></td></tr>
<tr><td colspan=3><center><img src="images/Sequence.png" width=815></center></td></tr>
<tr><td colspan=3><br><br></td></tr>
<tr><td width=30></td><td>Data on this page is described in the following publication:<br></td><td width=30></td></tr>
<tr><td width=30></td><td><blockquote>Yonatan Bisk, Daniel Marcu, and William Wong. <b><a href="2016-AAAI-Wksp.pdf">Towards a Dataset for Human Computer Communication via Grounded Language Acquisition</a></b>. In the <i>Proceedings of the AAAI 2016 Workshop on Symbiotic Cognitive Systems</i></blockquote></td><td width=30></td></tr>
<tr><td width=30></td><td>Preliminary Models on this data are described in the following forthcoming publication:<br></td><td width=30></td></tr>
<tr><td width=30></td><td><blockquote>Yonatan Bisk, Deniz Yuret, and Daniel Marcu. <b><a href="2016-NAACL.pdf">Natural Language Communication with Robots</a></b>. <i>Proceedings of the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics 2016</i></blockquote></td><td width=30></td></tr>
<tr><td width=30></td><td><br>All data takes the form of Problem-Solution Sequences (PSSs), like the one pictured above.  A series of images containing blocks in a 3D environment are rearranged to accomplish some goal.  The initial dataset release focuses on drawing 100 digits from the MNIST corpus, which have been downsampled to required 20 or fewer blocks.  The data was generated via Amazon's Mechanical Turk and annotators were asked to provide directions (as they would to a friend) to help complete the task.  This task might be the movement of a single block or the completion of a sequence of actions.  No restrictions were placed on the language used by annotators.  This leads to lots of ambiguity in the phrasing of similar actions and in the task of grounding the specific entities being referenced.</td><td width=30></td></tr>
<tr><td width=30></td><td><br><br></td><td width=30></td></tr>
<tr><td width=30></td><td>
<center><table cellpadding=10>
<tr><td><b>Raw Data</b></td><td><b>JSON</b></td><td><b>Images</b></td><td><b>Decoration</b></td><td><b>Dimensions</b></td></tr>
<tr><td>MNIST patterns</td><td><a href="BlockWorld/MNIST/trainset.json.gz">Train</a>/<a href="BlockWorld/MNIST/devset.json.gz">Dev</a>/<a href="BlockWorld/MNIST/testset.json.gz">Test</a></td><td><a href="BlockWorld/MNIST/trainset.img.tar">Train</a>/<a href="BlockWorld/MNIST/devset.img.tar">Dev</a>/<a href="BlockWorld/MNIST/testset.img.tar">Test</a></td><td>Logos & Digits</td><td>2D</td></tr>
<tr><td>Random </td><td><a href="BlockWorld/Random/trainset.json.gz">Train</a>/<a href="BlockWorld/Random/devset.json.gz">Dev</a>/<a href="BlockWorld/Random/testset.json.gz">Test</a></td><td><a href="BlockWorld/Random/trainset_random.img.tar">Train</a>/<a href="BlockWorld/Random/devset_random.img.tar">Dev</a>/<a href="BlockWorld/Random/testset_random.img.tar">Test</a></td><td>Blank</td><td>3D</td></tr>
</table></center>
<br><br>Images are only necessary if vision algorithms are to be employed.  Otherwise, the location and ID of all blocks are in the JSONs (<1Mb vs 360Mb for images)
<br><br>
<h3>Code</h3>
Conveniently manipulating the data or training models: <a href="https://github.com/ybisk/GroundedLanguage">https://github.com/ybisk/GroundedLanguage</a>
<br>Our simulator/environment: <a href="https://github.com/danielmarcu/ISI-CWIC/">https://github.com/danielmarcu/ISI-CWIC/</a>
<h3>FAQ</h3>
<b>Block Decoration:</b> Each sequence (JSON in the files) has a field labeled "decoration" which takes the values logo/digit/blank.
<ul>
<li><b>Blank</b> blocks have nothing drawn on their sides.</li>
<li><b>Digit</b> blocks have their ID (the numbers 1-20) written on every side.</li>
<li><b>Logo</b> blocks have a brand associated with every ID.  The following brands align alphabetically to the indices in order:<br> <tt>adidas, bmw, burger king, coca cola, esso, heineken, hp, mcdonalds, mercedes benz, nvidia, pepsi, shell, sri, starbucks, stella artois, target, texaco, toyota, twitter, ups</tt></li>
</ul>
<b>Block ordering:</b> The states data-structure in our JSONs refer to a sequence of <i>(x,y,z)</i> coordinates.  The ordering of this array aligns with the alphabetical ordering of logos of the numbers 1 through 20.
<br><br>
<b>A0/A1/A2:</b> A0 refers to single actions, A1 to short sequences and A2 to annotations of the full sequences.
<br><br>
</td><td width=30></td></tr>
</table>
</center>
</body>
</html>



<html>
<head>
<title>Blocks</title>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-8024377-7', 'auto');
  ga('send', 'pageview');
</script>
</head>
<body bgcolor=808080>
<center>
<br><br>
<table width=815 bgcolor=FFFFFF>
<tr><td colspan=3><br><center><h1 style="font-family:courier">BLOCKS Data Set</h1></center></td></tr>
<tr><td colspan=3 >
<ul style="list-style: none; padding: 0; margin: 0; display: flex; justify-content: space-around;">
<li><h3><a style="text-decoration:  none;" href="#DATA">Data</a></h3>
<li><h3><a style="text-decoration:  none;" href="#PUBLICATIONS">Publications/Code</a></h3>
<li><h3><a style="text-decoration:  none;" href="#FAQ">FAQ</a></h3>
</ul>
</td></tr>

<tr><td colspan=3><center><img src="images/Sequence.png" width=815></center></td></tr>


<tr><td width=30></td><td>
<br>
<h2 id="DATA">Data</h2>
All data takes the form of Problem-Solution Sequences (PSSs), like the one pictured above.  A series of images containing blocks in a 3D environment are rearranged to accomplish some goal.  The initial dataset release focuses on drawing 100 digits from the MNIST corpus, which have been downsampled to required 20 or fewer blocks.  The data was generated via Amazon's Mechanical Turk and annotators were asked to provide directions (as they would to a friend) to help complete the task.  This task might be the movement of a single block or the completion of a sequence of actions.  No restrictions were placed on the language used by annotators.  This leads to lots of ambiguity in the phrasing of similar actions and in the task of grounding the specific entities being referenced.</td><td width=30></td></tr>
<tr><td width=30></td><td></td><td width=30></td></tr>
<tr><td width=30></td><td>
<center><table cellpadding=10>
<tr><td><b>Raw Data</b></td><td><b>JSON</b></td><td><b>Images</b></td><td><b>Decoration</b></td><td><b>Dimensions</b></td></tr>
<tr><td>MNIST patterns</td><td><a href="BlockWorld/MNIST/trainset.json.gz">Train</a>/<a href="BlockWorld/MNIST/devset.json.gz">Dev</a>/<a href="BlockWorld/MNIST/testset.json.gz">Test</a></td><td><a href=https://github.com/groundedlanguage/groundedlanguage.github.io/tree/master/BlockWorld/MNIST/trainset>Train</a>/<a href=BlockWorld/MNIST/devset.img.tar>Dev</a>/<a href=BlockWorld/Random/testset.img.tar>Test</a></td><td>Logos & Digits</td><td>2D</td></tr>
<tr><td>Random </td><td><a href="BlockWorld/Random/trainset.json.gz">Train</a>/<a href="BlockWorld/Random/devset.json.gz">Dev</a>/<a href="BlockWorld/Random/testset.json.gz">Test</a></td><td><a href=BlockWorld/Random/trainset_random.img.tar>Train</a>/<a href=BlockWorld/Random/devset_random.img.tar>Dev</a>/<a href=BlockWorld/Random/testset_random.img.tar>Test</a></td><td>Blank</td><td>3D</td></tr>
<tr><td>Logo Version 2 </td><td><a href="BlockWorld/Version2/trainset.json.gz">Train</a>/<a href="BlockWorld/Version2/devset.json.gz">Dev</a>/<a href="BlockWorld/Version2/testset.json.gz">Test</a></td><td><a href=https://github.com/groundedlanguage/groundedlanguage.github.io/tree/master/BlockWorld/Version2/trainset>Train</a>/<a href=BlockWorld/Version2/devset.img.tar>Dev</a>/<a href=https://github.com/groundedlanguage/groundedlanguage.github.io/tree/master/BlockWorld/Version2/testset>Test</a></td><td>Logos</td><td>3D</td></tr>
</table></center>
Images are only necessary if vision algorithms are to be employed.  Otherwise, the location and ID of all blocks are in the JSONs (<1Mb vs 360Mb for images)
<br><br>


<tr><td width=30></td><td>
<br>
<h2 id="PUBLICATIONS">Publications & Code</h2>
<tr><td width=30></td><td>
<b>Data Paradigm</b>
<blockquote style="font-size:14px;margin: 5 40;">Yonatan Bisk, Daniel Marcu, and William Wong. <div style="float: right; clear: right;"><a href="https://github.com/danielmarcu/ISI-CWIC/">[Turk Code]</a></div></a>
<br><a href="2016-AAAI-Wksp.pdf">Towards a Dataset for Human Computer Communication via Grounded Language Acquisition</a>
</blockquote></td><td width=30></td></tr>


<tr><td width=30></td><td>
<b>Baselines</b>
<blockquote style="font-size:14px;margin: 5 40;">Yonatan Bisk, Deniz Yuret, and Daniel Marcu. <div style="float: right; clear: right;"><a href="https://github.com/ybisk/GroundedLanguage">[Model Code]</a></div>
<br><a href="2016-NAACL.pdf">Natural Language Communication with Robots</a> <i>NAACL 2016</i>
</blockquote></td><td width=30></td></tr>

<tr><td width=30></td><td>
<b>RL + Simulator</b> 
<blockquote style="font-size:14px;margin: 5 40;">Dipendra Misra, John Langford, and Yoav Artzi.<div style="float: right; clear: right;"><a href="https://github.com/clic-lab/blocks">[Simulator + Models]</a></div>
<br><a href="http://yoavartzi.com/pub/mla-emnlp.2017.pdf">Mapping Instructions and Visual Observations to Actions with Reinforcement Learning</a></b>. <i>EMNLP 2017</i>
</blockquote></td><td width=30></td></tr>

<tr><td width=30></td><td>
<b>Improvements to Baselines</b> 
<blockquote style="font-size:14px;margin: 5 40;">Bedrich Pisl and David Marecek<div style="float: right; clear: right;"><a href="errors.txt">[Data errors]</a></div>
<br><a href="https://aclanthology.info/pdf/W/W17/W17-2806.pdf">Communication with Robots using Multilayer Recurrent Networks</a>
</blockquote></td><td width=30></td></tr>

<tr><td width=30></td><td>
<b>Blank Blocks Results</b> <a span style="color: #009900;">(new)</a>
<blockquote style="font-size:14px;margin: 5 40;">Hao Tan and Mohit Bansal
<br><a href="https://arxiv.org/abs/1707.03804">Source-Target Inference Models for Spatial Instruction Understanding</a> AAAI 2018
</blockquote></td><td width=30></td></tr>


<tr><td width=30></td><td>
<b>3D Data w/ Rotations and Interpretable Model</b> <a span style="color: #009900;">(new)</a>
<blockquote style="font-size:14px;margin: 5 40;">Yonatan Bisk, Kevin Shih, Yejin Choi, and Daniel Marcu<div style="float: right; clear: right;"><a href="https://github.com/ybisk/GroundedLanguage">[Model Code]</a></div>
<br><a href="http://yonatanbisk.com/papers/2018-AAAI.pdf">Learning Interpretable Spatial Operations in a Rich 3D Blocks World</a> AAAI 2018
</blockquote></td><td width=30></td></tr>







<tr><td width=30></td><td>
<br>
<h2 id="FAQ">FAQ</h2>
<b>Block Decoration:</b> Each sequence (JSON in the files) has a field labeled "decoration" which takes the values logo/digit/blank.
<ul>
<li><b>Blank</b> blocks have nothing drawn on their sides.</li>
<li><b>Digit</b> blocks have their ID (the numbers 1-20) written on every side.</li>
<li><b>Logo</b> blocks have a brand associated with every ID.  The following brands align alphabetically to the indices in order:<br> <tt>adidas, bmw, burger king, coca cola, esso, heineken, hp, mcdonalds, mercedes benz, nvidia, pepsi, shell, sri, starbucks, stella artois, target, texaco, toyota, twitter, ups</tt></li>
</ul>
<b>Block ordering:</b> The states data-structure in our JSONs refer to a sequence of <i>(x,y,z)</i> coordinates.  The ordering of this array aligns with the alphabetical ordering of logos of the numbers 1 through 20.
<br><br>
<b>A0/A1/A2:</b> A0 refers to single actions, A1 to short sequences and A2 to annotations of the full sequences.
<br><br>
</td><td width=30></td></tr>
</table>
</center>
</body>
</html>

